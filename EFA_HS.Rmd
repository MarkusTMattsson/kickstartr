---
title: "EFA in R"
author: "MM"
date: "7 Jan 2016"
output: html_document
---

Let's find out how to do exploratory factor analysis (EFA) using R! In case someone needs a refresher, EFA is a technique related to psychological measurement. Because latent variables (factors), such as intelligence or extraversion, cannot be directly observed, they are measured using tests or questionnaires. If subtests or individual question items form groups that are highly correlated with each other, and less correlated with the other subtests / items, then we infer that their correlations are caused by an unobserved variable having an effect on all of them. The idea can perhaps be demonstrated by the picture by Kimmo Vehkalahti:

![](mittausmalli.gif)


Let's use the HolzingerSwineford1939 data set from the lavaan package and begin by getting acquainted with the data:


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lavaan)
library(psych)
library(semPlot)
library(ggplot2)
library(reshape2)
```

```{r}
?HolzingerSwineford1939
```

So as we see, the data set contains cognitive ability test scores from two schools. Let's first select data from one of the schools and perform EFA on it. We will later do a confirmatory factor analysis (CFA) using the data from the other school. Let's also form a new data set that contains only the cognitive ability test score variables (x1-x9) - that's the new data set Pasteur.x in the code chunk below:

```{r}
Pasteur <- with(HolzingerSwineford1939,HolzingerSwineford1939[school=="Pasteur",])
Pasteur.x <- Pasteur[grep("x[0-9]",names(Pasteur))] # I demonstrate using command "grep" in here, even though it's by no means necessary. grep is a command for finding text strings, a little bit like typing "Ctrl+F" in a text processor. Inside the square brackets we have grep("x[0-9]", names(Pasteur).  Here we use grep to find all text strings that contain the character "x" followed by any number from zero to nine. It searches for these text strings in the variable names of our new dataset "Pasteur". Then, the Pasteur data set is indexed using the results of grep: Pasteur[grep("x[0-9]",names(Pasteur))]. We would get the same result by saying simply: Pasteur[7:15]

describe(Pasteur.x)
boxplot(Pasteur.x)
summary(Pasteur.x)

```


The variables seem to be on the same 0-10 scale with no obvious outliers or weird values
Still, it's a 

```{r}
######################## A VERY IMPORTANT point to remember ################

# To first check how missing values are coded and recode to NA if necessary! 

# In this data, there are no missing values
# Missing values being empty cells would also be fine
# But sometimes missing data can be coded with, say, a large negative value
# like -999. In that case, we'd need to remember to recode those values to 
# empties, like so:

Pasteur.x[Pasteur.x == -999] <- NA

####################### \VIP ######################################
```

Let's draw one more picture of the data. First, we calculate the correlation matrix for the variables in Pasteur.x: there are nine variables, so we get a 9x9 correlation matrix. Then, we want to produce a plot of the correlation matrix so that the strength of a correlation between two variables is represented by the depth of color (deeper green = stronger correlation). 

Do you remember how to arrange the data for these kinds of plots in ggplot? The idea was to have the variables that go on x-axis and y-axis side by side in the data (on the same row), together with the value to be plotted there. So we need to transform the data from wide format (correlation matrix) to long format. For me, the easiest way to do this is to use the command "melt". In lecture 1.3.3. Esko used the reshape package, that's a matter of personal preference, and you can use that as well if it feels more natural to you.

```{r}
round(cor(Pasteur.x),2) # correlations, rounded here to two decimal precision
Pastcor <- melt(cor(Pasteur.x)) # transform to long format.
head(Pastcor,20)
```

I'll demonstrate the use of ggplot here. If you're not interested in this stuff, you can just skip to factor analysis. The basic idea of creating the plot that we want (it's called a "heatplot") is as follows (we use the geom called "geom_tile"):

```{r}
ggplot(Pastcor, aes(x = Var1,y = Var2)) + 
geom_tile(aes(fill=value))
```

So we are putting Var1 on x-axis and Var2 on y-axis (the first line of code above). Then, we choose to plot these as something called "tiles" - they are just the colorful boxes that you can see. In ggplot, the "what we are plotting as" is called the aesthetic of the plot - or aes in short. We make the color of the tile depend on what is found in the variable called "value" in the dataset we are working with (Pastcor).

We could have chosen to plot the value as something else as well. For instance, we could have chosen "geom_text" to plot it as text. "geom_text" needs at least the parameter "label", which in this example is what is found in the variable "value", rounded to two decimal places. I also make the size of text depend on the absolute value of the correlation by saying size = "abs(value)". Then, small correlations are shown as small text - handy!

```{r}
ggplot(Pastcor, aes(Var1,Var2)) + 
geom_text(aes( label=round(value,2), size=abs(value) ))
```

We might want to fine-tune our plot in many ways. I'll show some of these next.

First, we could remove the texts "Var1" and "Var2" from where they are next to x-axis and y-axis. We do this by assigning the value "NULL" to x-axis label (xlab) and y-axis label (ylab)

```{r}
ggplot(Pastcor, aes(x = Var1,y = Var2)) + 
geom_tile(aes(fill=value))  + xlab(NULL) + ylab(NULL) 
```

The colors of the plot can also be changed easily. I happen to like the green color more than the default blue, so I choose the colors of the plot by using "scale_fill_gradient". Remember you can find information on all these by asking for it, for example: ?scale_fill_gradient.

```{r}
ggplot(Pastcor, aes(x = Var1,y = Var2)) + 
geom_tile(aes(fill=value))  + xlab(NULL) + ylab(NULL) +
scale_fill_gradient(low = "white",high="forestgreen")
```


There's still one thing that's bothering me: it seems that the variables in the plot are in the wrong order. It might be nice to have them in the reverse order, with x1 on top and x9 on the bottom. That turns out to be a little bit involved to accomplish, but I'll walk you through it. 

By default, ggplot orders the x-variable and the y-variable in an ascending order on the axis. What we need to do is reverse the y-variable, and that is done by declaring Var2 as a variable of type factor (y= factor(Var2)) and then telling the program to reverse the levels of that factor (levels = rev(levels(Var2))). It's a little bit complicated operation, so let's check that we get the correct results by also including the value of the correlation coefficient in the tile using geom_text. We might further want to fine-tune the graph by adjusting the legends on the right side, but for the time being, we'll content ourselves with this result. Finally, let's also check that the results are correct by comparing those in the heatplot with the actual correlation matrix.

It's a good exercise to play around with the settings of ggplot, go ahead and try it using this data or your own!

```{r}
ggplot(Pastcor, aes(x = Var1,y = factor(Var2, levels=rev(levels(Var2))))) + 
geom_tile(aes(fill=value))  + xlab(NULL) + ylab(NULL) +
scale_fill_gradient(low = "white",high="forestgreen") + 
geom_text(aes(size=abs(value),label=round(value,2)))  

cor(Pasteur.x)
```

It's actually a nice picture! Variables x4-x6 correlate the highest, while the next highest correlations are a little bit difficult to tell. Do you know when in agent and police movies the scientists press the "enhance" button to bring clarity to absolute messes of pictures? Let's see if we have the button in R and play around with the limits option a little:

```{r}
ggplot(Pastcor, aes(x = Var1,y = factor(Var2, levels=rev(levels(Var2))))) + 
geom_tile(aes(fill=value))  + xlab(NULL) + ylab(NULL) +
scale_fill_gradient(low = "white",high="forestgreen", limits=c(0.26,1)) 
```

That's a bit uglier, but more informative! Perhaps there are three groups of variables, x1-x3, x4-x6 and x7-x9. Still, x1 seems to correlate with almost everything, and x9 also has correlations with x1 and x3. 
Let's see what factor analysis says about the data. There are many functions for factor analysis in R.
Base R has factanal, in which only Maximum Likelihood estimation is available. We'll use the function *fa* from the package *psych*

Let's first run parallel analysis (PA) to have an idea of the number of factors to extract. And actually, even before that, a small detour for you to understand PA better. You may be familiar with the eigenvalue-greater-than-one criterion for the number of factors.

PA is related to that idea, replacing the threshold of eigenvalue = 1 with that obtained from calculating eigenvalues from independent normal variables, i.e. random data. Doing so accounts for sampling variation: even if on the population level the variables are really uncorrelated, slight correlations may emerge due to sampling variation.

Let's demonstrate the idea by creating independent identically distributed (iid) normal variables and calculating the eigenvalues of their correlation matrix:

```{r}
normals <- matrix( rnorm(20*10,mean=0,sd=1), 20, 10)

# So the eigenvalues are:

eigen(cor(normals))

# And the largest eigenvalue

max(eigen(cor(normals))$values)
```

The largest of which are clearly above 1!

[On eigenvalues, a beautiful demo: ](http://setosa.io/ev/eigenvectors-and-eigenvalues/)

Anyhow, in a covariance matrix, the eigenvalues signify the amount of variance along dimensions of 1) largest spread in the data 2) second largest spread in the data, etc, [nicely demonstrated here:](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)

So, parallel analysis tells you whether there is more variance in your data,  along the dimensions of largest variance, than in comparable random data

So finally, parallel analysis:

```{r}
fa.parallel(Pasteur.x)
```

So we'll try factor analysis with three factors (so far with no factor rotation):

```{r}
fa.Pasteur <- fa(Pasteur.x,nfactors = 3,rotate = "none")
fa.Pasteur
```

On the first line of the output, we see that the function carries out minres factor analysis (i.e. ordinary least squares as estimation method, trying to minimize the residual correlation between the variables)

We can do principal axis factoring by saying:

```{r}
fa.Pasteur <- fa(Pasteur.x,nfactors = 3,rotate = "none",fm="pa")
fa.Pasteur
```

In the output, h2 is the communality (how much variation in the variable is due to the factors), u2 the uniqueness. So we can see that in some variables, more than 50 % variance is explained by the factor solution, while in variable x2 only 26 % is.

Let's try with an oblique rotation (oblimin):

```{r}
fa.Pasteur <- fa(Pasteur.x,nfactors = 3,rotate = "oblimin",fm="pa")
print(fa.Pasteur, sort=TRUE)
```

Seems like we have a good, simple structure, with items grouped as x1,x2,x3; x4,x5,x6; x7,x8,x9. 
x1 and x9 have a crossloading on another factor perhaps worth noting. The factors can be named by checking what's in the variables. These can be found by saying, again, ?HolzingerSwineford1939. x1-x3 seem to be related to visual processing, so we'll name the factor "Visual ability", x4-x6 have something to do with language processing, so we'll name that one "Textual ability", and the remaining three all require fast information processing, so we'll call that one "Processing speed". The factors seem moderately correlated, so perhaps it's a good idea to indeed use the oblique rotation - or to do bifactor analysis, with a separate general factor (g-factor).

Oh, in the previous output, the last column, "com", is the complexity score, i.e. the number of non-zero loadings of the item on the factors. Low scores correspond with a simple factor structure, which is something we are usually looking for.

You can also examine the solution by calling plot on the factor analysis solution:
```{r}
plot(fa.Pasteur)
```


And a factor loading diagram can be obtained by:

```{r}
fa.diagram(fa.Pasteur,cut = 0.01)
```

The structure matrix (correlations between the factors and variables) resides in:

```{r}
fa.Pasteur$Structure
```

There is also a procedure called the Schmid-Leiman transformation, which estimates the effect of a general factor (g-factor) together with the original factors - nicely implemented in the psych package:

```{r}
schmid(cor(Pasteur.x),nfactors = 3,fm = "pa")
```
