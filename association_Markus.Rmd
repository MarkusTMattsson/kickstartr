---
title: "Testing for association between two continous variables"
author: "Esko, commented by Markus"
date: "18.08.2016"
output: html_document
---


Dataset
-------------------
We will load the dataset created in section _Merging two data frames_, where we combined the dataset containing game performance data with that containing working memory test data. 

```{r}
game.wmc <- read.table("kr_gamewmc.txt", header=T)
```


Question: Is accuracy in the game associated with working memory capacity?
--------------------

Let's calculate _accuracy_ anew:
```{r}
game.wmc$accuracy <- game.wmc$Phit + game.wmc$PcorrectRejection
```

In order to visualize an association between two continous variables, a scatter plot would be perfect. 

```{r}
plot(game.wmc$pcu_score, game.wmc$accuracy) 
```

We can also calculate Pearson correlation (the default for the function _cor_). 

```{r}
cor(game.wmc$pcu_score, game.wmc$accuracy) 
```
But there were two missing values in the working memory test. We can either remove them, or instruct cor function to use only complete observations.
```{r}
cor(game.wmc$pcu_score, game.wmc$accuracy, use="complete.obs") 
```
With cor.test, it is possible to calculate the significance of the correlation, also. Note that missing values are removed by default.
```{r}
cor.test(game.wmc$pcu_score, game.wmc$accuracy)  
```

We could also use a linear regression to predict accuracy based on the working memory score. 

```{r}
fm <- lm(accuracy ~ pcu_score, game.wmc)
summary(fm)
```
The function _summary_ is useful for inspecting the model. The fitted model can be also given to plot command, which produces plots related to regression diagnostics. 
```{r}
plot(fm)
```

Finally, we could calculate the F-statistics for all the terms using the _anova_ function. In this case, the F statistics were already available in the summary because the model had only a single predictor. 
```{r}
anova(fm)
```

Beware of R's anova
---------------------
If we have a model with two or more predictors, the anova function is a bit complicated for user accustomed to SPSS.


Kirjoitin allaolevan tekstin 2*2 anovan näkökulmasta, mutta tässä tietysti on kyse kovarianssianalyysista. Ymmärsin tämän vasta tekstin kirjoitettuani. Tuo Zhanin teksti on hyvä, eikä asia ole minusta mitenkään ilmiselvä edes kategoristen selittäjien tapauksessa. Kun toinen selittäjä on jatkuva ja toinen kategorinen, en ole itse asiassa varma, miten painotetuista summista pitäisi ajatella, so. mistä eri neliösummien väliset erot johtuvat. Yksi vaihtoehto voisi olla kategorisoida jatkuva selittäjä esimerkin vuoksi, mutta toisaalta muuttujien dikotomisointikin on epäilyttävää puuhaa, sillä se saattaa tuottaa harhaanjohtavia tuloksia... 

First, the _anova_ function uses type I Sum of Squares. So when the data is unbalanced, the order in which the predictors are included in the model matters. The first predictor explains most of the variation, the second explains the portion of variance left over from the first predictor etc. In the case of an unbalanced design, this amounts to comparing group means weighted by their individual sample sizes, which is something that the researcher is not usually interested in. For a fine discussion of these issues, see the [presentation by Zhan](http://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/InteractionsAndTypesOfSS.pdf). 
```{r}
##################################################################################
# Before calculating anovas using R, set the default contrasts to orthogonal, 
# if you want to obtain results that are comparable to those produced by SPSS. 
#
# Especially, if you want to use type III Sum of Squares like in SPSS by default, 
# you MUST use orthogonal contrasts! Otherwise your results may be wrong. 
##################################################################################
options(contrasts = c("contr.sum", "contr.poly")) 


# main effect and the interaction
fm1 <- lm(accuracy ~ pcu_score + age_grp + pcu_score:age_grp, game.wmc) 
anova(fm1)

# this a shorthand for main effects and interaction
fm2 <- lm(accuracy ~ pcu_score * age_grp, game.wmc) 
anova(fm2)

# here the order is different
fm3 <- lm(accuracy ~ age_grp * pcu_score, game.wmc)
anova(fm3)

```
The results with the third model are different from the first and second (which are actually exactly the same models), because of type I Sum of Squares.

In order to use type III Sum of Squares, where each term is adjusted for all the terms, it is easiest to use function Anova from package car.

```{r}
# I repeat this here. Never forget, especially with type=3
# Mut eihän tota contrastsia tarvii määritellä jos käyttää car-paketin Anovaa??
options(contrasts = c("contr.sum", "contr.poly")) 

require(car)
Anova(fm2, type=3)
Anova(fm3, type=3)
```

Btw, do you recognize something funny here? Neither one of the main effects is significant, even though if tested separately they are. We have actually run into problems with multicollinearity here. Therefore, after controlling for age\_grp, there is nothing to explain with pcu\_score anymore, and vice versa. This is also the problem with type III Sum of Squares. If you use them blindly, you may get misleading results.

Millä lailla tämä on ongelma? Eikö kaikki tämä riipu hiukan siitä mitä halutaan testata? Kovarianssianalyysista tulee mieleen, että joskushan sen käyttöä 
